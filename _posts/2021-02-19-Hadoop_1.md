---
layout: post
title:  "[hadoop] hadoop_1"
categories: hadoop
comments: true


---

# Hadoop



불과 10년 전만 해도 DB가 다루는 데이터 양이 수천만건에 달하면 대용량 데이터를 처리할 줄 안다고 자랑했습니다.

 2~3년 사이 DB가 감당해야 할 정보의 양은 급속도로 증가했습니다.

수억 개에 달하는 빅데이터를 처리할 줄 알아야합니다. 

텍스트 위주의 정보가 과거에 많았던 반면 이제는 그림과 동영상 위주의 정보가 증가하고 있습니다.

 텍스트 중심의 정형 데이터를 처리했던 관계형 데이터베이스로는 그림과 동영상 위주의 비정형 데이터를 감당할 수 없습니다.

 

처음에는 스토리지를 추가하는 방식으로 늘어나는 데이터를 감당했습니다.

DAS, NAS, SAN 등의 스토리지를 추가해가며 빅데이터를 감당했고, 이를 데이터베이스가 처리하였으나 한계에 부딪혔습니다. 스토리지만 늘린다고 해서 빅데이터를 처리할 수 있는 것이 아니었기 때문입니다

 

대용량 병렬 처리(MPP; Massively Parallel Processing) 방법이 등장했고 프로그램을 여러 부분으로 나눠 여러 프로세스가 각 부분을 동시에 수행시킬 수 있게 했습니다.하나의 프로그램을 수행하는데 수백 또는 수천개의 프로세스를 이용할 수 있게 했습니다.그러나 성능 개선도 정형 데이터를 처리할 때는 효율적이었지만 폭증하는 데이터를 처리하는 데는 비용이 효율적이지 못했습니다.고객이 감당하기에 투자하는 자금이 너무나 비쌌던 것입니다

 

#### 이때 혜성처럼 등장한 것이 바로 하둡입니다.

 

하둡(hadoop)은 대용량의 데이터 처리를 위해 개발된 오픈소스 소프트웨어입니다.

(open-source software)

 

구글의 분산 파일 시스템 기능은 하둡 분산 파일 시스템(HDFS, Hadoop Distributed File System), 구글의 맵리듀스는 하둡 맵리듀스(Hadoop MapReduce), 구글의 빅테이블은 Hbase가 각각 담당하고 있습니다.

 

 

하둡은 핵심 구성 요소인 분산 파일 시스템과 맵리듀스 이외에 다양한 기능을 담당하는 시스템으로 구성되어 있습니다. 하둡 프로그램을 쉽게 처리하기 위한 솔루션으로 피그(Pig)와 하이브(Hive)가 있습니다.

 

피그는 야후에서 개발되었는데 현재는 하둡 프로젝트에 포함되어 있습니다. 피그는 데이터를 적재·변환하고 결과를 정렬하는 과정을 쉽게 처리하기 위해 만든 프로그램 언어입니다. 하이브는 하둡을 데이터웨어하우스(DW)로 운영할 수 있게 해주는 솔루션이다 패이스북에서 개발한 하이브는 관계형 데이터베이스에서 사용하는 SQL과 유사한 질의 언어(querry language)의 특징을 가지고 있습니다.

 

맵리듀스는 쉽게 말하면 데이터를 분산시켜 처리한 뒤 하나로 합치는 기술입니다.

 

하둡은 MPP와 달리 사용이 편리하다 - 개발자들이 각 데이터를 분산시키고 합치는 일을 할 필요 없이 하둡의 맵리듀싱 기술이 이를 자동적으로 지원

 

‘하둡 에코시스템’이 등장

대용량 데이터를 저장할 수 있는 NoSQL, 데이터베이스인 HBase, SQL과 비슷한 쿼리로 분석을 수행할 수 있는 Hive와 Pig, 그리고 분산 관리 시스템 개발이 가능한 Zookeeper 등이 포함됐습니다.

 

비싼 장비를 도입해 빅데이터를 분석할 필요가 없는 중소기업을 중심으로 하둡 사용이 증가했습니다

 

IBM, 오라클, 테라데이타 같은 분석 전문 솔루션 업체들은 빅데이터 분석에 솔루션 사용 비용, 서버 비용 등 엄청난 초기 자본금을 요구했습니다

폐쇄적으로 소스를 제공해 한번 도입한 이후에는 다른 대안으로 옮겨가기 쉽지 않다는 문제가 있습니다

 

빅데이터를 쉽고, 간편하고, 편리하고, 빠르게 분석할 만한 기술로 하둡 만한 게 없습니다

하둡을 통해 빅데이터를 처리하면 안정성 면에서 일반적인 데이터베이스를 이용한 분석보다 떨어집니다. 

 

데이터베이스가 99.9999%의 고가용성을 자랑한다면, 하둡은 99.99%의 고가용성을 자랑합니다. 소수점 차이지만 이 차이는 금융권 등 중요한 정보를 처리하고 분석하는 기업들에게는 꽤 중요한 의미로 다가갑니다.

따라서 데이터베이스 업체들은 하둡을 외면하기 이전에 하둡을 품어 새롭게 고객들은 공략하기로 마음을 바꿨습니다. 

하둡은 IBM DB2, EMC 그린플럼, 오라클 빅데이터 어플라이언스 등 데이터베이스 업체들이 출시하는 거의 모든 제품에 적용됐습니다.

 

 

**개요** **–** **하둡을 쓰게 된 이유**

````hadoop
: 10TB크기의 데이터를 소팅하는 문제가 있다고 하자. 이를 한 대의 서버에서 소팅하려면 2~3일정도 걸릴것이다. 이는 서버 한대로 처리할 수 없는 규모이다. 따라서 100대 서버를 가지고 하둡 클러스터로 처리하면 30~40분이면 충분하다. 즉, 하둡은 대용량 데이터 분산처리 프레임웍이다. 
````



Q. 왜 굳이 하둡 클러스터로 처리하나? 기존의 Oracle같은 DBMS분산처리로는 안되나?

````hadoop
오라클 같은 RDBMS는 분산 환경을 염두에 두지 않고 한 대의 서버만을 생각해서 만들어진 소프트웨어이다. 기본적으로 이런 소프트웨어들은 데이터 처리용량을 늘릴려면 메모리를 추가한다던지 CPU나 디스크를 더 장착하는 방식으로 서버의 리소스를 추가해야 한다. 이런 방식을 스케일업(Scale-Up)이라고 한다. 
````



Q. RDBMS들의 성능 향상 기법이 스케일업이라면 하둡은?

````hadoop
하나의 서버에 더 많은 리소스를 붙여서 용량을 키우는 것이 아니라 서버 자체를 더 추가함으로써 전체 시스템의 용량을 키우는 방식을 스케일아웃(Scale-out) 이라한다. NoSQL이나 하둡 등의 분산 환경 시스템에서 시스템의 용량을 증대시키기 위한 방법이다. 스케일업처럼 고가의 장비보다는 저가의 장비를 여러 대 사용하는 방식을 주로 택한다. 
````





**빅데이터 처리 시스템의 전체적인 구성**

![hadoop_img_1](https://user-images.githubusercontent.com/38201897/108467162-1c5fd400-72c8-11eb-9cad-edf0e3251635.png)

빅데이터가 있고, 데이터를 처리하는 HDFS(Hadoop Distributed File System)과 MapReduce가 있다. 그리고 처리된 데이터를 보기 위한 도구인 NoSQL로 구성된다. 



Q. R이란걸 들어봤는데 R은 그럼 무엇인가요?

````hadoop
 R은 데이터 시각화 모듈이다. 처리 모듈을 통해 분석된 데이터를 시각화 해주기 위한 모듈이다. 주로 처리된 데이터를 가지고 그래프를 그리거나 해서 데이터의 특성을 파악하기 위해 사용된다. 
````

데이터 저장/처리 모듈 (HDFS & MapReduce)

빅데이터를 저장하고 처리하는 역할을 담당하는 것이 빅데이터 시스템의 핵심 브레인이라고 할 수 있는 하둡이다. 

 

하둡은 크게 두가지 모듈로 구성된다.

**HDFS – 하둡 분산 파일 시스템**

**MapReduce – 분산 처리 시스템**

앞서 수집된 데이터들이 HDFS에 저장되고 HDFS에 저장된 데이터들은 MapReduce 를 통해 처리된다. 

하둡은 대용량 데이터의 배치 프로세싱을 위한 모듈이다. 

**실시간 데이터 분석용도가 아니다.** 

: 데이터를 분석하기 위해서는 하둡에서 대용량 배치 프로세싱 후 생성된 데이터를 가지고 분석용 툴을 이용하여야 한다. 

데이터 분석 방법에는 기존 관계형 데이터베이스 하둡으로 처리된 데이터 크기가 상대적으로 작다면 RDBMS에 처리된 데이터를 넣은 후 데이터베이스의 기능을 이용하여 데이터를 분석한다. 

 

 

NoSQL

일반적으로 빅데이터에서 하둡으로 처리되어 만들어진 데이터는 그 크기가 굉장히 크며 스키마가 고정되어있지 않는 등 기존의 관계형 데이터베이스에 적합하지 않다. 

NoSQL이 의미하듯이 No Structured Query Language 즉, 정형화된 RDBMS의 테이블 형식이 아닌 데이터 형식에서 데이터를 CRUD하고 싶을 경우 NoSQL을 사용한다. 

분산환경을 위해 설계되었다. 

기존의 RDBMS보다 처리할 수 있는 양이나 트래픽이 훨씬 거대하다.

 

 

MongoDB, HBase, Cassandra 등이 있다.

주로 빅데이터 시스템 처리/분석을 위해 하둡과 NoSQL을 사용한다. 

검색엔진 RDBMS나 NoSQL처럼 키/밸류의 형태의 데이터를 처리하기보다 더 복잡한 형태로 데이터에 액세스하여야 할 경우 사용한다. 

검색엔진 오픈소스 프로젝트로는 Lucene, Solr, ElasticSearch가 있다.

 

 

하둡을 대규모 데이터 처리에 활용한 대표적인 사례로는 《뉴욕타임스》가 있습니다. 《뉴욕타임스》는 1851년부터 1980년까지의 기사 1100만 건을 PDF로 변환하는 대규모 프로젝트를 수행하면서 하드웨어와 소프트웨어를 신규로 구매하는 대신 아마존 EC2와 S3, 그리고 하둡(Hadoop) 플랫폼을 활용했습니다.

 

하루만에 작업을 완료하고 지불한 비용은 1450달러에 불과했습니다

하둡은 공개용 소프트웨어이기 때문에 무료로 이용할 수 있다는 장점이 있습니다. 

현재 인터넷 환경에서 오픈소스로 제공되는 개발 도구로는 LAMP(Linux, Apache, MySQL, PHP/Python)가 있습니다. 운영체제인 리눅스(Linux), 웹 서버인 아파치(Apache), 데이터베이스는 MySQL, 개발언어인 PHP/Python을 사용하면 저렴한 비용으로 시스템을 개발할 수 있습니다



*** 설치**



1. VMware 12 설치

2. C:\hadoop에 폴더 CentOS_master 생성

3. 리눅스 설치

4. VMware Tool

5. JDK

http://java.oracle.com

 

가. hadoop 계정 로그인

나. Firefox열고 JDK 다운로드 - 저장

다. /home/hadoop/다운로드 폴더에 저장된다

라. JDK 설치는 압축만 풀면 됩니다

  root계정으로 압축풀고 hadoop 사용자에게 읽기 권한을 줘야 합니다

 

[hadoop@localhost 바탕화면]$ su -

암호: 

[root@localhost ~]# cd /usr/local 

[root@localhost local]# pwd

/usr/local

[root@localhost local]# tar -xvf /home/hadoop/다운로드/jdk-8u15         1-linux-x64.tar.gz

[root@localhost local]# ls -l /usr/local - 설치되었나 확인

[root@localhost local]# chown -R hadoop:hadoop /usr/local/jdk1.8.0_111/

 

 

 

6. Eclipse

http://www.eclipse.org

 

가. hadoop 계정 로그인

나. Firefox열고 Eclipse 다운로드 - 저장

다. /home/hadoop/다운로드 폴더에 저장된다

라. Eclipse 설치는 압축만 풀면 됩니다

  root계정으로 압축풀고 hadoop 사용자에게 읽기 권한을 줘야 합니다

 

[root@localhost local]# pwd

/usr/local

[root@localhost local]# ls -l /home/hadoop/다운로드

[root@localhost local]# tar -xvf /home/hadoop/다운로드/eclipse-java-neon-2-linux-gtk-x86_64.tar.gz

[root@localhost local]# ls -l /usr/local - 설치되었나 확인

[root@localhost local]# ls -l ./eclipse/

[root@localhost local]# chown -R hadoop:hadoop /usr/local/eclipse/

 

7. Hadoop

http://archive.apache.org/dist/hadoop/common/

 

가. hadoop 계정 로그인

나. Firefox열고 hadoop 다운로드 - 저장

다. /home/hadoop/다운로드 폴더에 저장된다

라. hadoop 설치는 압축만 풀면 됩니다



  root계정으로 압축 풀고 hadoop 사용자에게 읽기 권한을 줘야 합니다

 

[root@localhost local]# pwd

/usr/local

[root@localhost local]# ls -l /home/hadoop/다운로드

[root@localhost local]# tar -xvf /home/hadoop/다운로드/hadoop-2.7.3.tar.gz

[root@localhost local]# ls -l ./hadoop-2.7.3/

[root@localhost local]# chown -R hadoop:hadoop hadoop-2.7.30/

 

8. CentOS OpenJDK 삭제

관리자 모드에서 실행해야 한다 

 

[root@localhost local]# pwd

/usr/local

[root@localhost local]# java -version

 

가. 설치되어 있는 OpenJDK 파일명이 나온다

 \# rpm -qa | grep jdk

java-1.7.0-openjdk-1.7.0.51-2.4.4.1.el6_5.x86_64

 

 

나. 나타나는 파일명을 복사 후

 \# yum remove java-1.7.0-openjdk-1.7.0.51-2.4.4.1.el6_5.x86_64

 

다. OpenJDK를 다 지우면 에러가 뜬다

[root@localhost local]# java -version

-bash: /usr/bin/java: 그런 파일이나 디렉터리가 없습니다

 

그러면 JDK, 하둡 경로를 등록하기위해 .bash_profile에 내용을 추가해야 한다

 

9. bashrc 환경설정

.bash_profile 파일은 모드 노드에 동일하게 설정되어 있어야 합니다.

 

[hadoop@localhost 바탕화면]$ cd

[hadoop@localhost ~]$ pwd

/home/hadoop

[hadoop@localhost ~]$ java -version

bash: java: command not found

 

[hadoop@localhost ~]$ cat .bash_profile

[hadoop@localhost ~]$ vi .bash_profile ( I는 입력 ) (esc -> : -> q! or wq)

 

export PATH=$PATH:$HOME/bin

export JAVA_HOME=/usr/local/jdk1.8.0_121

export HADOOP_INSTALL=/usr/local/hadoop-2.7.4

export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_INSTALL/bin

 

구분자 :

 

[hadoop@localhost ~]$ source .bash_profile

[hadoop@localhost ~]$ hadoop version

[hadoop@localhost ~]$ java -version

[hadoop@localhost ~]$ javac -version

 

10. ip확인

/root에서 /etc/hosts 설정

 

두 대의 호스트로 테스트 할 수 있다

먼저 자신의 IP부터 확인 한다

 

[hadoop@localhost 바탕화면]$ su -

암호:

[root@localhost ~]# ifconfig

[root@localhost ~]# vi /etc/hosts:

[root@localhost ~]#

192.168.121.133 master

192.168.111.128 backup

192.168.111.129 slave1

192.168.111.128 slave2

 

두 대의 호스트로 테스트한다

호스트 1 : master, backup, slave2

호스트 2 : slave1

 

[root@master 바탕화면]$ ping slave1

 

11. 방화벽 설정 – iptables

 

[hadoop@master 바탕화면]$ su - 

[root@master ~]# vi /etc/sysconfig/iptables         

\# Firewall configuration written by system-config-firewall

\# Manual customization of this file is not recommended.

*filter

:INPUT ACCEPT [0:0]

:FORWARD ACCEPT [0:0]

:OUTPUT ACCEPT [0:0]

-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

-A INPUT -p icmp -j ACCEPT

-A INPUT -i lo -j ACCEPT

-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT

​                     /16-> 192.168만 고정하겠다.

-A INPUT -s 192.168.0.0/16 -d 192.168.0.0/16 -j ACCEPT 

-A OUTPUT -s 192.168.0.0/16 -d 192.168.0.0/16 -j ACCEPT

 

-A INPUT -j REJECT --reject-with icmp-host-prohibited

-A FORWARD -j REJECT --reject-with icmp-host-prohibited

COMMIT

​              방화벽 고쳤으니 재시작

[root@master ~]# service iptables restart       iptables 적용

iptables: 체인을 ACCEPT 규칙으로 설정 중 : filter    [ OK ]

iptables: 방화벽 규칙을 지웁니다:            [ OK ]

iptables: 모듈을 언로드하는 중:             [ OK ]

iptables: 방화벽 규칙 적용 중:              [ OK ]

 

[root@master ~]# ping slave1

 

12. ssh rsa키를 이용하여 비밀번호 입력 없이 로그인하기

서로 간에 검증된 키(authorized_key, 암호)를 미리 주고받아 ssh 접속 시 비밀번호를 입력하지 않고 바로 접속하는 방식을 말한다

또한 authorized_key를 주고받지 않은 다른 계정은 접속하지 못하도록 하여 보안성을 높일 수 있다

authorized_key는 권한을 644로 수정 한다

모든 노드에 작성 한다

 

가. 먼저 공개키를 생성한다 

[hadoop@master ~]$ ssh-keygen -t rsa

Generating public/private rsa key pair.

Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): 

Enter passphrase (empty for no passphrase): 

Enter same passphrase again: 

Your identification has been saved in /home/hadoop/.ssh/id_rsa.

Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.

The key fingerprint is:

c2:69:3b:36:4d:4f:bc:18:df:3f:b0:00:02:f1:38:25 hadoop@master

The key's randomart image is:

````image
+--[ RSA 2048]----+
|  E..     |
|  .=      |
|  o..     |
|   o....    |
|   =.S.o   |
|   . = *.o.  |
|   = o +..o  |
|   . o  ... |
|       .. |
+-----------------+
````

나. .ssh가 있나 확인

[hadoop@master 바탕화면]$ cd /home/hadoop

[hadoop@master ~]$ ls -la (.ssh 만 있나 확인)

[hadoop@master ~]$ cd .ssh

 

[hadoop@master .ssh]$ ls -l

합계 12

-rw-------. 1 hadoop hadoop 1675 2014-12-23 11:46 id_rsa //개인키

-rw-r--r--. 1 hadoop hadoop 394 2014-12-23 11:46 id_rsa.pub //공개키

 

다. master 공개키를 authorized_keys에 추가

[hadoop@master .ssh]$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys

[hadoop@master .ssh]$ ls -l

합계 16

-rw-r--r--. 1 hadoop hadoop 394 2014-12-23 11:50 authorized_keys

-rw-------. 1 hadoop hadoop 1675 2014-12-23 11:46 id_rsa

-rw-r--r--. 1 hadoop hadoop 394 2014-12-23 11:46 id_rsa.pub

 

라. slave1에서도 개인키/공개키 생성

[hadoop@slave1 ~]$ ssh-keygen -t rsa

 

마. slave1 공개키를 master의 authorized_keys 파일에 추가

backup, slave2의 공개키도 추가한다

 

[hadoop@master .ssh]$ ssh hadoop@slave1 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

The authenticity of host 'slave1 (192.168.164.131)' can't be established.

RSA key fingerprint is 8e:6b:98:d8:cd:c2:a4:00:25:ea:32:28:02:76:ba:b9.

Are you sure you want to continue connecting (yes/no)? yes

Warning: Permanently added 'slave1,192.168.164.131' (RSA) to the list of known hosts.

hadoop@slave1's password:

 

[hadoop@master .ssh]$ ssh hadoop@backup cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

[hadoop@master .ssh]$ ssh hadoop@slave2 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

 

바. 모든 node에 공개키 재분배 한다

모든 노드에서 서로의 공개키를 공유한다.

[hadoop@master .ssh]$ scp authorized_keys hadoop@slave1:~/.ssh/

[hadoop@master .ssh]$ scp authorized_keys hadoop@backup:~/.ssh/

[hadoop@master .ssh]$ scp authorized_keys hadoop@slave2:~/.ssh/

 

[hadoop@master .ssh]$ ssh-add //master에서만 해주면 된다

Identity added: /home/hadoop/.ssh/id_rsa (/home/hadoop/.ssh/id_rsa)

 

사. 권한을 644로 수정

[hadoop@master .ssh]$ ls -la /home/hadoop/

[hadoop@master .ssh]$ chmod 644 ~/.ssh/authorized_keys

 

[실습]

[hadoop@master .ssh]$ ssh hadoop@master date

[hadoop@master .ssh]$ ssh hadoop@backup date

[hadoop@master .ssh]$ ssh hadoop@slave1 date

[hadoop@master .ssh]$ ssh hadoop@slave2 date

 

[hadoop@slave1 .ssh]$ ssh hadoop@master date

[hadoop@slave1 .ssh]$ ssh hadoop@backup date

[hadoop@slave1 .ssh]$ ssh hadoop@slave1 date

[hadoop@slave1 .ssh]$ ssh hadoop@slave2 date

 

[실습]

※ master에서 slave1으로 로그인

[hadoop@master ~]$ ssh slave1

Last login: Wed Feb 17 13:57:38 2016 from master

[hadoop@slave1 ~]$

※ slave1에서 master으로 로그인

[hadoop@slave1 ~]$ ssh master

hadoop@master's password: 

Last login: Wed Feb 17 14:55:15 2016 from slave1

[hadoop@master ~]$ 

13. 하둡 환경 설정 파일 수정

하둡이 실행하는 모든 프로세스에 적용되는 시스템 환경 값을 설정

이 스크립트를 실행한 후 다른 스크립트들이 실행된다.

 

설정 파일 위치 : **/usr/local/hadoop-2.7.3/etc/hadoop/**

 

가. hadoop-env.sh

하둡에게 JDK설치 경로 등록

 

[hadoop@master hadoop]$ cd /usr/local/hadoop-2.7.3/etc/hadoop/

[hadoop@master hadoop]$ ls –l

[hadoop@master hadoop]$ vi hadoop-env.sh  //환경 파일

export JAVA_HOME=/usr/local/jdk1.8.0_121

 

맨 마지막에 추가하세요

export HADOOP_OPTS="$HADOOP_OPTS-Djava.library.path=/usr/local/hadoop-2.7.3/lib/native"

 

나. masters

보조 네임노드를 실행할 서버 등록하는 파일

한 대로 하려면 localhost로 지정. (default로 지정되어 있음)

 

다. slaves

데이터 노드를 실행할 서버 설정

한 대로 하려면 localhost 지정. (default로 지정되어 있음)

데이터 노드가 여러 개이면 라인단위로 서버이름을 설정하면 된다

[hadoop@master hadoop]$ cat slaves

localhost

[hadoop@master hadoop]$ vi slaves

slave1

slave2

 

라. core-site.xml 파일 수정

로그파일, 네트워크 튜닝, I/O 튜닝, 파일 시스템 튜닝, 압축 등 하부 시스템 설정파일

core-site.xml 파일은 HDFS와 맵리듀스에서 공통적으로 사용할 환경정보 설정합니다. 

hadoopcore-1.x.x.jar 파일에 포함되어 있는 core-default.xml을 오버라이드 한 파일입니다. 

core-site.xml에 설정 값이 없을 경우 core-default.xml에 있는 기본 값을 사용합니다

공통 속성들에 대한 자세한 설명은 다음 주소를 참고하세요.

http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml

 

[hadoop@master hadoop]$ vi core-site.xml  

````config
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
         <name>hadoop.tmp.dir</name>
         <value>/usr/local/hadoop-2.7.3/tmp</value>
     </property>
</configuration>
````

 

**※** **하둡 분산 파일 시스템(HDFS: Hadoop Distributed File System)**

 

마. hdfs-site.xml 수정

데이터 저장 경로 변경

hdfs-site.xml 파일은 HDFS에서 사용할 환경 정보를 설정합니다. 

hadoop-core-2.2.0.jar 파일에 포함되어 있는 hdfs-default.xml을 오버라이드 한 파일입니다. 

hdfs-site.xml에 설정 값이 없을 경우 hdfs-default.xml에 있는 기본 값을 사용합니다.

 

HDFS 속성들에 대한 자세한 설명은 다음 주소를 참고하세요.

http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

 

[hadoop@master hadoop]$ vi hdfs-site.xml

````config
<configuration>
    <property>
        <name>dfs.replication</name>
         <value>1</value> //데이터를 1개만 복사:가상분산모드, 3일경우:완전분산모드
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.http.address</name>
        <value>master:50070</value>
    </property>
    <property>
        <name>dfs.secondary.http.address</name>
        <value>backup:50090</value>
    </property>
</configuration>
````

 

바. mapred-site.xml 파일 수정

mapred-site.xml 파일은 맵리듀스에서 사용할 환경정보를 설정합니다. 

hadoop-core-x.x.x.jar 파일에 포함되어 있는 mapred-default.xml을 오버라이드 한 파일입니다. 

mapred-site.xml에 설정 값이 없을 경우 mapred-default.xml에 있는 기본 값을 사용합니다.

만약 mapred-site.xml이 존재하지 않을 경우 mapred-site.xml.template를 복사하여 사용

 

[hadoop@master hadoop]$ cp mapred-site.xml.template mapred-site.xml

[hadoop@master hadoop]$ vi mapred-site.xml

````config
<configuration>

       <property>

              <name>mapreduce.framework.name</name>

              <value>yarn</value>

       </property>

</configuration>
````



사. yarn-site.xml

수정 안함, default설정 따름, 그런데 mapred-site.xml에서 yarn을 선택했을 경우 내용 추가

맵리듀스 프레임워크에서 사용하는 셔플 서비스를 지정한다

 

[hadoop@master hadoop]$ vi yarn-site.xml

````hadoop


<configuration>

       <property>

              <name>yarn.nodemanager.aux-services</name>

              <value>mapreduce_shuffle</value>

       </property>

       <property>

              <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>

              <value>org.apache.hadoop.mapred.ShuffleHandler</value>

       </property>

</configuration>
````



 

14. 다른 노드 설정파일 동기화

 

\* rsync

원격 서버의 파일을 네트워크를 거쳐서 전송하는 실행 소프트웨어

 

 

다른 노드에서

\# mkdir /usr/local/hadoop-2.7.3

\# chown -R hadoop:hadoop /usr/local/hadoop-2.7.3

 

마스터 노드에서

\# cd /usr/local/hadoop-2.7.3

\# rsync -av . hadoop@backup:/usr/local/hadoop-2.7.3

\# rsync -av . hadoop@slave1:/usr/local/hadoop-2.7.3

\# rsync -av . hadoop@slave2:/usr/local/hadoop-2.7.3

 

\# cd /usr/local/hadoop-2.7.3/etc/hadoop

\# rsync -av . hadoop@backup:/usr/local/hadoop-2.7.3/etc/hadoop

\# rsync -av . hadoop@slave1:/usr/local/hadoop-2.7.3/etc/hadoop

\# rsync -av . hadoop@slave2:/usr/local/hadoop-2.7.3/etc/hadoop

 

**15. 네임노드 초기화**

컴퓨터를 하드 디스크를 사시거나 USB를 사시면 그걸 이용하기 위해서는 먼저 format를 합니다.

하드디스크 초기화라고 생각하셔도 되는데 Hadoop 또한 구성 시 처음 하실 일은 format하는 일입니다.

 

네임노드는 최초 한번만 실행하면 됨

에러메시지가 있다면 환경설정파일이 잘못된 것임. 

확인하고 수정한 다음 다시 실행 시킬 것



[hadoop@master ~]$ cd /usr/local/hadoop-2.7.3/bin

[hadoop@master bin]$

[hadoop@master bin]$ hdfs namenode -format

 

가. 프로세스 실행

[hadoop@master sbin]$ pwd

/usr/local/hadoop-2.7.3/sbin

[hadoop@master sbin]$ ./start-dfs.sh

[hadoop@master sbin]$ ./start-yarn.sh

 

[hadoop@master sbin]$ jps

4147 DataNode   -----------------------> slave2

12373 NameNode

12703 SecondaryNameNode  ----------> backup

12851 ResourceManager

13451 Jps

9590 NodeManager

13392 JobHistoryServer

[hadoop@slave1 sbin]$ jps

6001 DataNode

6103 NodeManager

6350 Jps

 

브라우저에서도 확인

http://master:50070 또는

http://master:50070/dfshealth.html 실행후 파일 시스템 상태 보여야 함

 

1.x의 JobTracker는 http://master:8088/cluster 에서 확인할 수 있음

 

콘솔에서도 확인

[hadoop@master hadoop]$ hdfs dfsadmin -refreshNodes

Refresh nodes successful

[hadoop@master sbin]$ hdfs dfsadmin –report

 

**★** **eclipse 실행**

[hadoop@master ~]$ mkdir jar

 

이클립스 실행

[hadoop@master eclipse]$ pwd

/usr/local/eclipse

[hadoop@master eclipse]$ ./eclipse

Workspace : /home/hadoop/workspace

 

[실습]

Project : hadoop

 

★ 필요한 JAR

/usr/local/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar

/usr/local/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar

/usr/local/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core.2.7.3.jar

/usr/local/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j.1.2.17.jar

**★** **HDFS FS shell명령어**

 ````shell
1 폴더생성 : hadoop fs -mkdir [폴더명]

2. 파일/폴더 삭제 : hadoop fs -rm [파일명]

​          hadoop fs -rm -r [폴더명]

3. 파일 복사 : hadoop fs -cp [폴더명] [폴더명/파일명]

4. 리스트 보기 : hadoop fs -ls /

5. 파일 내용 보기 : hadoop fs*-cat [파일명]

6. 파일 올리기 : hadoop fs-put [로컬경로파일명] [하둡경로파일명]

하둡경로를 생략하면 default 디렉토리가 된다
 ````

[hadoop@master ~]$ hadoop fs –ls abc.txt

[hadoop@master ~]$ hadoop fs –ls ← /user/hadoo의 위치

-rw-r--r--  1 hadoop supergroup     14 2017-01-15 23:02 abc.txt

 

[hadoop@master ~]$ hadoop fs –ls /

drwxr-xr-x  - hadoop supergroup      0 2017-01-15 23:02 /user

 

[hadoop@master ~]$ hadoop fs -mkdir /test

 

[hadoop@master ~]]$ hadoop fs –ls /

drwxr-xr-x  - hadoop supergroup     0 2017-01-16 00:43 /test

drwxr-xr-x  - hadoop supergroup     0 2017-01-15 23:02 /user

 

리눅스 파일

[hadoop@master ~]$ vi apple.txt

 

[hadoop@master ~]$ hadoop fs –put apple.txt ← 리눅스 파일을 하둡에 올리기

[hadoop@master ~]$ hadoop fs –ls

-rw-r--r--  1 hadoop supergroup     38 2017-01-16 00:50 apple.txt

-rw-r--r--  1 hadoop supergroup     14 2017-01-15 23:02 abc.txt

 

[hadoop@master ~]$ hadoop fs –put apple.txt /

 

 

 

[hadoop@master ~]$ hadoop fs -ls /

-rw-r--r--  1 hadoop supergroup     38 2017-01-16 00:50 /apple.txt

drwxr-xr-x  - hadoop supergroup     0 2017-01-16 00:43 /test

drwxr-xr-x  - hadoop supergroup     0 2017-01-15 23:02 /user

 

[hadoop@master ~]$ hadoop fs -ls /user/hadoop

-rw-r--r--  1 hadoop supergroup     18 2017-01-16 11:18 /user/hadoop/apple.txt

-rw-r--r--  1 hadoop supergroup     24 2017-01-16 10:50 /user/hadoop/abc.txt

[hadoop@master ~]$ hadoop fs -cat /apple.txt

[hadoop@master ~]$ hadoop fs -cat /user/hadoop/abc.txt

 

[hadoop@master ~]$ hadoop fs -cp /apple.txt /pear.txt

 

[hadoop@master ~]$ hadoop fs -ls /

Found 4 items

-rw-r--r--  1 hadoop supergroup     38 2017-01-16 00:50 /apple.txt

-rw-r--r--  1 hadoop supergroup     38 2017-01-16 01:02 /pear.txt

drwxr-xr-x  - hadoop supergroup     0 2017-01-16 00:43 /test

drwxr-xr-x  - hadoop supergroup     0 2017-01-15 23:02 /user

 

[hadoop@master ~]$ hadoop fs -cat /pear.txt

 

[hadoop@master ~]$ hadoop fs -rm /pear.txt

 

[hadoop@master ~]$ hadoop fs -rm -r /test

  

★ 그 외 명령어들

````command
cat
chgrp
chmod
chown
copyFromLocal
copyToLocal
count
cp
du
dus
expunge
get
getmerge
ls
lsr
mkdir
moveFromLocal
moveToLocal
mv
put
rm
rmr
setrep
stat
tail
test
text
touchz
````

-----

Node라는것은 "서버 1대"를 의미합니다

 

NameNode - 네임노드

​      \- DataNode에 실제 파일의 Meta정보들을 저장하는 곳

​      \- 실제 파일을 저장하지 않음

SecondaryNode - 세컨너리 노드

​        \- NameNode가 장애 발생하면 차후 복구 시 활용되는 정보

DataNode - 실제 Data가 저장하는공간 (html, avi, mp3, pdf, nosql 등등 )

 

Hadoop 분산 파일 시스템(HDFS)에서는 64MB로 짜른 것이 **블록(Block)**입니다.

![hadoop_img_2](https://user-images.githubusercontent.com/38201897/108467169-1d910100-72c8-11eb-92dd-bea5a9c0699f.png)

150MB짜리 1.avi라는 동영상 파일이 있습니다

1.avi 동영상을 HDFS안에 저장 할 경우 우선 150MB짜리 1.avi를 64MB로 나눕니다.

150MB / 64MB = 2.3입니다. 

즉 150MB 짜리 파일1개를 3개의 파일로 만들 것이며, 3개로 쪼겐다(split)라고도 합니다. 이 쪼개진 파일 하나 **하나를** **Block(블록) 또는 청크(chunk)**라고도 말합니다. 위의 그림에 1-1, 1-2, 1-3이 Block입니다.

 

만약 서버 1대에 600MB를 읽기/쓰기 할 때와 600MB를 10개로 나누어 서버10대가 읽고/쓰고 한다 라고 하면 서버 1대당 64MB를 읽기/쓰기 하는 것입니다

 

 

DFS는 복제 정책을 운영자가 세울 수 있습니다. 

HDFS로 구성된 Node에 블록파일을 2개씩 복제, 3개씩 복제, 4개씩 복재 할 수 있습니다.

그렇기 때문에 특정 서버에서 장애가 발생해도 다른 서버에 블록파일이 존재하기 때문에 서비스에 영향이 없습니다. 

이것을 **No Single Point of Failure** 라고 합니다.

 

기본적으로 HDFS는 2가지 형태의 서버(Node)가 있습니다. 

**NameNode**와 **DataNode**입니다.

 

NameNode는 크게 2가지로 나눠집니다.

NameNode(Master개념)와 SecondaryNameNode로 나눠집니다. 

NameNode에는 실제 파일이 존재 하지 않습니다

NameNode(Master개념)는 DataNode에 저장되어있는 파일의 정보를 가지고 있습니다. 

이것을 **Meta정보**라고 합니다.

실제적으로 tree구조로 파일 정보를 가지고 있습니다.

![hadoop_img_3](https://user-images.githubusercontent.com/38201897/108467170-1d910100-72c8-11eb-9e23-82d8d88d641a.png)

NameNode(Master개념)은 실제 파일을 저장하고 있지는 않지만 DataNode에 저장된 파일의 정보를 저장하고 있습니다. 

“책에서는 네임노드는 파일시스템의 네임스페이스를 관리하다”라고도합니다.

또한 저장하는 공간은 메모리영역에 저장 하게 되며 그 이유는 Client가 파일을 찾을때는 읽고, 쓰기 할때 빠른 DataNode를 찾아가기 위해서입니다.

위에서 메모리 영역에 저장되었다고 하지만 실제로 메모리의 경우 컴퓨터가 리부팅 될 경우 사라지기 때문에 메모리 영역 외에 Local 디스크(하드디스크)에도 메터정보를 유지하게 되는데 이Meta정보는 로컬디스크에 **네임스페이스 이미지(Namespace image)와 에디트 로그(edit log)형태로 지속적으로 저장**합니다.

그렇기 때문에 네임노드는 주어진 파일에 대한 모든 블록이 있는 데이터 노드을 알고 있습니다

NameNode가 DataNode의 실제 Mata정보를 기억하고 있기 때문에 파일을 찾기가 가능합니다. 

그러나 NameNode가 에러/장애가 발생했을 때는 HDFS로 구성된 모든 파일을 사용할 수 없습니다. 

DataNode에 실질적으로 파일이 저장되어 있다고 하더라도 찾을 수 있는 방법이 없습니다.

 

그래서 이런 장애를 방지하기 위해 Secondary Namenode가 존재합니다.

**SecondaryNameNode는 Master개념의 Namenode시 장애가 발생했을 때 대처용이 아닙니다**.

 

HDFS의 Secondary Namenode는 위와 같은 기능을 하고 있습니다. **NameNode가 장애 시 다시 실행했을 때 Secondary에 저장된 정보를 가지고 복구할 수 있도록 제공하는 서버입니다. 그렇기 때문에 NameNode(Master개념)에** **namespace image****와** **edit log, 저널 파일****등 필요한 정보를 주기적으로 실행하여 저장 합니다**

 

 

DataNode는 실제 파일이 저장되는 공간입니다. 

동영상 파일이던, txt파일, excel파일, pdf, Hbase의 DB정보, mp3 파일 등등 모든 파일을 저장하게 되어 있습니다. 

**DataNode의 Block단위로 저장되며 기본 Block는 64MB입니다.** 

또한 이런 블록들은 복제(4번) 정책에 의해 다른 DataNode로 Copy를 합니다.

 

Block들은 내부 정책에 의해 여러 DataNode에 저장이 되는데, 특정 Node(서버)가 장애가 발생했을 때 다른 Node들의 Block정보를 읽으므로서 서비스에 이상이 없이 구동이 됩니다.

이것을 **No Single Point of Failure**입니다. 

NameNode는 장애발생시 여러 방법으로 대처방법을 찾아야 하지만 DataNode는 그럴 걱정이 없습니다.

 

기본적으로 DataNode의 모든 서버들은 NameNode와 통신을 하며, Client의 요청으로 Block파일들을 읽고를 수행합니다.